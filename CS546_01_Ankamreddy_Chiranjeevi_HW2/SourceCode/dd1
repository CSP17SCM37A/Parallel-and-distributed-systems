Initially,we need to read Matrix size,random Seed and outputfile from Command line Arguments.and intilaize inputs of A,B and X.then,Initilize Thread variables and  create the number of workers(threads) whiich includes main thread.Now,call gauss() to parallieze algorithm using Pthreads.Immediatly after creation, all workers start waiting on the sync condtion variable. This allows them to wakeup if there is work available.
                                                now,we have to do gauss elimination using worker barrier since work gets executed from here.we have sync_lock which is used for barrier and row lock used for scheduling.and worker barrier take care of synchronization .worker is going to Start processing only when all the children and main thread are ready.They will work in  parallel using dynaamic scheduling.and we join and synchronize workers to do work.finally,work gets done on one instace wher worker barrier synchronize with other threads.and join all the workers together before they terminated.


 The MPI gaussian elimination algorithm is as follows: 
Supposing we have A and B where A*X = B, for every iteration:
1) Broadcast the row that we are focusing in this iteration ( A[norm] ) and also the value in the array B
2) Divide the workload among processes. Every process will handle a few rows because there are no dependencies between the operations
3) Scatter the data, diving it between processes. Every process can calculate how many rows is going to receive.
4) Perform the sequential algorithm only in the assigned rows.
5) Process #0 now collects the data using Recv, while the other processes send the data using Isend. This is faster than using Gatherv
6) Data is directly put in the array A
7) Go to the next iteration
Go to the function gaussElimination() for more details
